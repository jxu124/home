<!DOCTYPE HTML>
<html>
	<head>
		<title>Jie Xu</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<link rel="stylesheet" href="assets/css/main.css" />
		<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap-icons@1.10.0/font/bootstrap-icons.css">
		<noscript><link rel="stylesheet" href="assets/css/noscript.css" /></noscript>
	</head>
	<body class="is-preload">

		<!-- Wrapper -->
			<div id="wrapper">

				<!-- Header -->
					<header id="header" class="alt">
						<span class="logo" ><img src="images/logo.svg" alt="" width="120" /></span>
						<h1>Jie Xu</h1>
						<p>
							许杰 <br/>
							Ph.D Student <br/>
							Control Science and Engineering <br/>
							Xi'an Jiaotong University <br/>
						</p>
					</header>

				<!-- Nav -->
					<nav id="nav">
						<ul>
							<li><a href="#intro" class="active">Introduction</a></li>
							<li><a href="#bg" class="active">Background</a></li>
							<li><a href="#proj">Projects</a></li>
							<li><a href="#pub">Publications</a></li>
							<li><a href="#pat">Patents</a></li>
							<li><a href="#awa">Awards</a></li>
						</ul>
					</nav>

				<!-- Main -->
					<div id="main">

					<!-- Introduction -->
						<section id="intro" class="main">
							<div class="spotlight">
								<div class="content">
									<header class="major">
										<h2>About me</h2>
									</header>
									<p>
										许杰，博士研究生，就读于西安交通大学人工智能学院，研究方向为机器人和人机交互，师从
										<a href="https://gr.xjtu.edu.cn/web/zeuslan">兰旭光教授</a>
										。近期专注于机器人多模态交互模型与VLA机器人操作模型研究。
									</p>
									<p>
										I'm a soon-to-be PhD graduate from the College of Artificial 
										Intelligence, Xi'an Jiaotong University. My research focuses 
										on robotics and human-robot interaction, 
										supervised by 
										<a href="https://gr.xjtu.edu.cn/web/zeuslan">Prof. Xuguang Lan</a>
										. My recent research interests 
										lie in multimodal interaction models and VLA robotic manipulation models.
									</p>
									<a class="icon brands bi-envelope-fill" href="mailto:xujiexx@gmail.com"> Email: <u>xujiexx@gmail.com</u></a>
								</div>
								<span class="image">
									<img src="https://cache-r2-home.jxu124.com/files/jpg/ava01.jpg" type="image/jpg">
								</span>

								
								
							</div>
						</section>

					<!-- Projects Section -->
						<section id="bg" class="main special">
							<header class="major">
								<h2>Background</h2>
							</header>
							<div class="table-wrapper" style="text-align: left;">
								<table>
									<thead>
										<tr>
											<th>Time</th>
											<th>Institution</th>
											<th>Position</th>
										</tr>
									</thead>
									<tbody>
										<tr>
											<td>2023.2-2025.6</td>
											<td>字节跳动（北京）Seed Robotics</td>
											<td>研究实习</td>
										</tr>
										<tr>
											<td>2018.9-2025.9</td>
											<td>西安交通大学 人工智能学院</td>
											<td>控制科学与工程</td>
										</tr>
										<tr>
											<td>2014.9-2018.6</td>
											<td>湖南大学 电气与信息工程学院</td>
											<td>自动化</td>
										</tr>
									</tbody>
								</table>
							</div>
						</section>

					<!-- Projects Section -->
						<section id="proj" class="main special">
							<header class="major">
								<h2>Projects</h2>
							</header>

							<h3>2024 &bullet; RoboICL VLA模型的上下文学习探索</h3>
							<blockquote style="text-align: left;">
								简介：该项目的任务任务是将上下文学习（In-Context Learning, ICL）应用于机器人操纵领域，难点在于此前相关研究未充分借鉴语言领域 ICL 的成功经验。为此，提出 RoboICL，这是一种自回归机器人基础模型，通过将视觉、语言和动作标记化为统一离散空间，基于视觉 - 语言数据集和机器人操纵数据集训练语言模型，并应用上下文微调，仅基于下一个标记预测进行训练。实验结果表明，ICL 显著提升了模型在基本操纵和泛化场景中的性能，RoboICL 达到了最先进的操纵性能水平。
							</blockquote>
							<picture>
								<source src="files/webp/roboicl.webp" class="rounded-video" type="image/webp">
								<source src="https://cache-r2-home.jxu124.com/files/webp/roboicl.webp" class="rounded-video" type="image/webp">
								<img src="https://cache-r2-home.jxu124.com/files/webp/roboicl.webp" class="rounded-video">
							</picture>
							<hr />

							<h3>2024 &bullet; SInViG 交互消歧模型的自我迭代进化方法</h3>
							<blockquote style="text-align: left;">
								简介：该项目的任务是要进一步解决人机交互中的语言歧义问题，难点在于日常环境中视觉输入复杂不可预测、交互开放且用户需求多样。为此，提出了 SInViG，一种基于自然语言的自进化交互式视觉代理，它通过多轮视觉语言对话来消除歧义，能自动从未标注图像和大语言模型中学习。效果是在多个交互式视觉基础基准上取得了最优成绩，获得了用户更多偏好，且在 Franka 机器人上的实验表明它能在复杂环境中遵循用户指令并自然交互。
							</blockquote>
							<video class="rounded-video" controls poster="https://cache-r2-home.jxu124.com/files/webp/sinvig_demo_video.webp">
								<source src="files/mp4/sinvig_demo_video.mp4" type="video/mp4">
								<source src="https://cache-r2-home.jxu124.com/files/mp4/sinvig_demo_video.mp4" type="video/mp4">
								<source src="https://home-1257933320.cos.ap-guangzhou.myqcloud.com/files/mp4/sinvig_demo_video.mp4" type="video/mp4">
							</video>
							<hr />

							<h3>2023 &bullet; TiO 交互式视觉定位方法及其机器人系统</h3>
							<blockquote style="text-align: left;">
								简介：该项目的任务是要实现机器人准确交互式视觉定位并为人类提供物体，难点在于自然语言可能存在歧义，需机器人主动收集信息来消除，而先前方法依赖预定义模板，在现实场景中性能不佳。为此，提出了 TiO 端到端系统，它基于视觉对话和定位的统一公式，可在大量公共数据上进行训练。实验结果表明，TiO 在 GuessWhat?! 和 InViG 基准测试中取得了新的最优性能，在 150 个具有挑战性的场景及真实机器人平台上的人机交互实验也显示出其对多样化视觉和语言输入具有卓越的通用性和高成功率。
							</blockquote>
							<video class="rounded-video" controls poster="https://cache-r2-home.jxu124.com/files/webp/icra2024_fin_20mb.webp">
								<source src="files/mp4/icra2024_fin_20mb.mp4" type="video/mp4">
								<source src="https://cache-r2-home.jxu124.com/files/mp4/icra2024_fin_20mb.mp4" type="video/mp4">
								<source src="https://home-1257933320.cos.ap-guangzhou.myqcloud.com/files/mp4/icra2024_fin_20mb.mp4" type="video/mp4">
							</video>
							<video class="rounded-video" controls poster="https://cache-r2-home.jxu124.com/files/webp/tio_23_1024.webp">
								<source src="files/mp4/tio_23_1024.mp4" type="video/mp4">
								<source src="https://cache-r2-home.jxu124.com/files/mp4/tio_23_1024.mp4" type="video/mp4">
								<source src="https://home-1257933320.cos.ap-guangzhou.myqcloud.com/files/mp4/tio_23_1024.mp4" type="video/mp4">
							</video>
							<hr />

							<h3>2022 &bullet; 运动预测的持续学习方法</h3>
							<blockquote style="text-align: left;">
								简介：该项目的任务是要实现人机交互中准确的概率性人体运动预测，难点在于现有算法依赖预收集数据，无法处理不熟悉的运动模式且未考虑协作方的实时响应。为此，提出一种针对运动预测模型的持续学习方法，通过贝叶斯神经网络安全收集在线交互数据，并利用经验重放和知识蒸馏提升模型能力，同时保留先前知识。实验结果表明，该方法在评测集上预测误差更低，能持续学习新运动模式且不遗忘已学知识，在真实场景中还能从零学习人体运动学模型，能有效保障人机交互安全。
							</blockquote>
							<video class="rounded-video" controls poster="https://cache-r2-home.jxu124.com/files/webp/202205.webp">
								<source src="files/mp4/202205.mp4" type="video/mp4">
								<source src="https://v.06dn.com/api/v3/slave/download/0/L3VwbG9hZHMvMzE3MDYvMjAyNTA4LzE4LzMxNzA2X0VSVWNkNUdIXzIwMjIwNS5tcDQ/202205.mp4?sign=bxNVRafXXQLM5V_z3JL6zk1BydtAzBQu0lnb7dvV7pw%3D%3A1755607569" type="video/mp4">
								<source src="https://cache-r2-home.jxu124.com/files/mp4/202205.mp4" type="video/mp4">
								<source src="https://home-1257933320.cos.ap-guangzhou.myqcloud.com/files/mp4/202205.mp4" type="video/mp4">
							</video>
							<hr />

							<h3>2021 &bullet; 人体运动学的不确定性建模</h3>
							<blockquote style="text-align: left;">
								简介：该项目针对确定性人体运动预测算法可能导致机器人决策风险的问题，提出基于贝叶斯神经网络的概率预测模型，通过生成多种未来运动并计算两种不确定性以提供最优预测，在基准数据集及人机交互场景中验证均表现良好，较之前的工作提升了交互效率与安全性。
							</blockquote>
							<video class="rounded-video" controls poster="https://cache-r2-home.jxu124.com/files/webp/202105_2529_v1.webp">
								<source src="files/mp4/202105_2529_v1.mp4" type="video/mp4">
								<source src="https://v.06dn.com/api/v3/slave/download/0/L3VwbG9hZHMvMzE3MDYvMjAyNTA4LzE4LzMxNzA2X1BQY1F2VjFJXzIwMjEwNV8yNTI5X3YxLm1wNA/202105_2529_v1.mp4?sign=pg6P7fs2kgHCweGn3qpEgVUj9nLbz1YjYvQdeiT0RuU%3D%3A1755607594" type="video/mp4">
								<source src="https://cache-r2-home.jxu124.com/files/mp4/202105_2529_v1.mp4" type="video/mp4">
								<source src="https://home-1257933320.cos.ap-guangzhou.myqcloud.com/files/mp4/202105_2529_v1.mp4" type="video/mp4">
							</video>
							<video class="rounded-video" controls poster="https://cache-r2-home.jxu124.com/files/webp/202105_2529_v2.webp">
								<source src="files/mp4/202105_2529_v2.mp4" type="video/mp4">
								<source src="https://v.06dn.com/api/v3/slave/download/0/L3VwbG9hZHMvMzE3MDYvMjAyNTA4LzE4LzMxNzA2X1lpQmpVSUNCXzIwMjEwNV8yNTI5X3YyLm1wNA/202105_2529_v2.mp4?sign=RdX_4Vgd2JsvGxPbeq24PJuWeliVChbx5W5biahZojY%3D%3A1755607536" type="video/mp4">
								<source src="https://cache-r2-home.jxu124.com/files/mp4/202105_2529_v2.mp4" type="video/mp4">
								<source src="https://home-1257933320.cos.ap-guangzhou.myqcloud.com/files/mp4/202105_2529_v2.mp4" type="video/mp4">
							</video>
							<hr />

							<h3>2017 &bullet; 全国电子设计竞赛 &bullet; 滚球控制系统</h3>
							<blockquote style="text-align: left;">
								简介：滚球控制系统赛题要求设计一套控制系统，通过控制边长 65cm 光滑正方形平板倾斜，使直径不大于 2.5cm 小球按指定要求在平板上 9 个外径 3cm 圆形区域间完成动作并计时显示；难点在于小球运动轨迹的精确控制，涉及复杂的机械设计与自动控制算法应用及视觉伺服控制应用等，且小球运动易受平板材质、支撑结构、摩擦力等多种因素影响 。
							</blockquote>
							<video class="rounded-video" style="object-fit: cover;" controls poster="https://cache-r2-home.jxu124.com/files/webp/201710.webp">
								<source src="files/mp4/201710.mp4" type="video/mp4">
								<source src="https://v.06dn.com/api/v3/slave/download/0/L3VwbG9hZHMvMzE3MDYvMjAyNTA4LzE4LzMxNzA2X3hDMGVpN0Z2XzIwMTcxMC5tcDQ/201710.mp4?sign=XAPGjDKWr_0Q1JaDpAOcXHkFNQ8ZCMu60EnYJhgETC4%3D%3A1755590265" type="video/mp4">
								<source src="https://cache-r2-home.jxu124.com/files/mp4/201710.mp4" type="video/mp4">
								<source src="https://home-1257933320.cos.ap-guangzhou.myqcloud.com/files/mp4/201710.mp4" type="video/mp4">
							</video>
							<hr />

							<h3>2017 &bullet; 全国工训竞赛 &bullet; 无碳小车越障竞赛</h3>
							<blockquote style="text-align: left;">
								简介：无碳小车越障赛题要求设计一种三轮结构小车，仅靠质量 1Kg、下降高度 400±2mm 的标准砝码提供的重力势能驱动行走及转向，且砝码全程随车载运，同时小车需具备可调节的转向控制机构，以适应不同间距障碍物场地，难点在于如何高效利用有限重力势能实现稳定行走与精准转向。
							</blockquote>
							<video class="rounded-video" controls poster="https://cache-r2-home.jxu124.com/files/webp/201705.webp">
								<source src="files/mp4/201705.mp4" type="video/mp4">
								<source src="https://v.06dn.com/api/v3/slave/download/0/L3VwbG9hZHMvMzE3MDYvMjAyNTA4LzE4LzMxNzA2X0tVSFgxaXUxXzIwMTcwNS5tcDQ/201705.mp4?sign=53MDkHNqx53IrUBXzZEfyyCE5zGb8uOHINb6jC5wULc%3D%3A1755607519" type="video/mp4">
								<source src="https://cache-r2-home.jxu124.com/files/mp4/201705.mp4" type="video/mp4">
								<source src="https://home-1257933320.cos.ap-guangzhou.myqcloud.com/files/mp4/201705.mp4" type="video/mp4">
							</video>

						</section>

					<!-- Publications Section -->
						<section id="pub" class="main special">
							<header class="major">
								<h2>Publications</h2>
							</header>

							<p style="text-align: left;">
								
								<b>SInViG: A Self-Evolving Interactive Visual Agent for Human-Robot Interaction. </b> <br/>
								<b>Jie Xu</b>, Hanbo Zhang, Xinghang Li, Huaping Liu, Xuguang Lan, Tao Kong. <br/>
								<b>ICRA Workshop 2024.</b> 
								[<a href="https://arxiv.org/abs/2402.11792" class="icon brands bi-file-text" target="_blank" rel="noopener noreferrer"> Paper</a>]
								[<a href="player.html?url=https://cache-r2-home.jxu124.com/files/mp4/sinvig_demo_video.mp4" class="icon brands bi-film" target="_blank" rel="noopener noreferrer"> Video</a>]
								<br/><br/>

								<b>Towards Unified Interactive Visual Grounding in The Wild. </b> <br/>
								<b>Jie Xu</b>, Hanbo Zhang, Qingyi Si, Yifeng Li, Xuguang Lan, Tao Kong. <br/>
								<b>ICRA 2024.</b> 
								[<a href="https://arxiv.org/abs/2401.16699" class="icon brands bi-file-text" target="_blank" rel="noopener noreferrer"> Paper</a>]
								[<a href="player.html?url=https://cache-r2-home.jxu124.com/files/mp4/icra2024_fin_20mb.mp4" class="icon brands bi-film" target="_blank" rel="noopener noreferrer"> Video</a>]
								[<a href="player.html?url=https://cache-r2-home.jxu124.com/files/mp4/tio_23_1024.mp4" class="icon brands bi-film" target="_blank" rel="noopener noreferrer"> Demo</a>]
								[<a href="https://jxu124.github.io/TiO" class="icon brands bi-globe" target="_blank" rel="noopener noreferrer"> Website</a>]
								[<a href="https://huggingface.co/jxu124/TiO" class="icon brands bi-code-slash" target="_blank" rel="noopener noreferrer"> Code</a>]
								<br/><br/>

								<b>Vision-Language Foundation Models as Effective Robot Imitators. </b> <br/>
								Xinghang Li, Minghuan Liu, Hanbo Zhang, Cunjun Yu, <b>Jie Xu</b>, Hongtao Wu, Chilam Cheang, Ya Jing, Weinan Zhang, Huaping Liu, Hang Li, Tao Kong. <br/>
								<b>ICLR 2024.</b> 
								[<a href="https://arxiv.org/abs/2311.01378" class="icon brands bi-file-text" target="_blank" rel="noopener noreferrer"> Paper</a>]
								[<a href="https://roboflamingo.github.io/" class="icon brands bi-globe" target="_blank" rel="noopener noreferrer"> Website</a>]
								[<a href="https://github.com/RoboFlamingo/RoboFlamingo" class="icon brands bi-code-slash" target="_blank" rel="noopener noreferrer"> Code</a>]
								[<a href="https://huggingface.co/roboflamingo/RoboFlamingo" class="icon brands bi-file-zip" target="_blank" rel="noopener noreferrer"> Model</a>]
								<br/><br/>
								
								<b>Experience Consistency Distillation Continual Reinforcement Learning for Robotic Manipulation Tasks. </b> <br/>
								Chao Zhao, <b>Jie Xu</b>, Ru Peng, Xingyu Chen, Kuizhi Mei, Xuguang Lan. <br/>
								<b>ICRA 2024.</b> 
								[<a href="https://ieeexplore.ieee.org/document/10611494" class="icon brands bi-file-text" target="_blank" rel="noopener noreferrer"> Paper</a>]
								[<a class="icon brands bi-film"> Video</a>]
								<br/><br/>

								<b>InViG: enchmarking Open-Ended Interactive Visual Grounding with 500K Dialogues. </b> <br/>
								Hanbo Zhang*, <b>Jie Xu</b>*, Yuchen Mo, Tao Kong. <br/>
								<b>CVPR Workshop 2024.</b> 
								[<a href="https://arxiv.org/abs/2310.12147" class="icon brands bi-file-text" target="_blank" rel="noopener noreferrer"> Paper</a>]
								[<a href="https://openivg.github.io/" class="icon brands bi-globe" target="_blank" rel="noopener noreferrer"> Code</a>]
								[<a href="https://huggingface.co/datasets/jxu124/invig" class="icon brands bi-file-zip" target="_blank" rel="noopener noreferrer"> Dataset</a>]
								<br/><br/>
								
								<b>A Continuous Learning Approach for Probabilistic Human Motion Prediction. </b> <br/>
								<b>Jie Xu</b>*, Shihong Wang*, Xingyu Chen, Jiahao Zhang, Xuguang Lan, Nanning Zheng. <br/>
								<b>ICRA 2022.</b> 
								[<a href="https://ieeexplore.ieee.org/document/9811906" class="icon brands bi-file-text" target="_blank" rel="noopener noreferrer"> Paper</a>]
								[<a href="player.html?url=https://cache-r2-home.jxu124.com/files/mp4/202205.mp4" class="icon brands bi-film"> Video</a>]
								<br/><br/>

								<b>Probabilistic Human Motion Prediction via A Bayesian Neural Network. </b> <br/>
								<b>Jie Xu</b>*, Xingyu Chen*, Xuguang Lan, Nanning Zheng. <br/>
								<b>ICRA 2021.</b> 
								[<a href="https://arxiv.org/abs/2107.06564" class="icon brands bi-file-text" target="_blank" rel="noopener noreferrer"> Paper</a>]
								[<a href="player.html?url=https://cache-r2-home.jxu124.com/files/mp4/202105_2529_v1.mp4" class="icon brands bi-film" target="_blank" rel="noopener noreferrer"> Video1</a>]
								[<a href="player.html?url=https://cache-r2-home.jxu124.com/files/mp4/202105_2529_v2.mp4" class="icon brands bi-film" target="_blank" rel="noopener noreferrer"> Video2</a>]
								<br/><br/>

								<b>EAN: Error Attenuation Network for Long-term Human Motion Prediction.</b> <br/>
								<b>Jie Xu</b>, Xuguang Lan, Jin Li, Xingyu Chen, Nanning Zheng. <br/>
								<b>CCHI 2019.</b> 
								[<a href="https://ieeexplore.ieee.org/document/8901951" class="icon brands bi-file-text" target="_blank" rel="noopener noreferrer"> Paper</a>]
								[<a class="icon brands bi-film"> Video</a>]
								<br/><br/>
							</p>
						</section>

					<!-- Get Started -->
						<section id="pat" class="main special">
							<header class="major">
								<h2>Patents</h2>

								<p>已授权的专利</p>
								<ol class="references" style="text-align: left;">
									<li>兰旭光, <b>许杰</b>, 王仕鸿, 陈星宇, 张家豪. 一种可持续学习的人体运动预测方法: ZL202210505137.7[P]. 2025-05.</li>
								</ol>
								<br/>

								<p>审查中的专利</p>
								<ol class="references" style="counter-reset: ref-counter 1; text-align: left;">
									<li>张翰博, <b>许杰</b>, 孔涛. 任务执行方法、装置、设备及计算机介质: CN202410145231.5[P]. 2024-04.</li>
									<li><b>许杰</b>, 张翰博, 李兴航, 孔涛. 用于从图像中识别对象的方法、装置、设备和介质: CN202410179054.2[P]. 2024-05.</li>
									<li>张翰博, <b>许杰</b>, 黎意枫, 孔涛. 任务执行方法、装置，设备及计算机介质: CN202410130685.5[P]. 2024-04.</li>
									<li>张翰博, <b>许杰</b>, 黎意枫, 孔涛. 模型训练方法、装置、设备及计算机介质: CN202311559285.8[P]. 2024-04.</li>
									<li> <b>Xu; Jie</b>, Zhang; Hanbo, Li; Xinghang, Kong; Tao. METHOD, APPARATUS, DEVICE AND MEDIUM FOR OBJECT RECOGNITION FROM IMAGE: US-20250259422-A1[P]. 2025-08.</li>
									<li> Zhang; Hanbo, <b>Xu; Jie</b>, Kong; Tao. METHOD, APPARATUS, DEVICE, AND COMPUTER MEDIUM FOR TASK EXECUTION: US-20250249581-A1[P]. 2025-08.</li>
									<li> ZHANG; Hanbo, <b>Xu; Jie</b>, Li; Yifeng, Kong; Tao. TASK EXECUTION METHOD AND APPARATUS, DEVICE, AND COMPUTER MEDIUM: US-20250242496-A1[P]. 2025-07.</li>
									
									<li>黎意枫, 张翰博, <b>许杰</b>, 孔涛. 数据处理方法、装置、设备及计算机介质: CN202311562350.2[P]. 2024-01.</li>
									<li>张翰博, 李兴航, 刘明桓, <b>许杰</b>, 吴弘涛, 荆雅, 郑子琳, 孔涛, 李航. 信息处理方法、任务执行方法、装置、设备及介质: CN202311280845.6[P]. 2024-01.</li>
									<li> LI; Yifeng, ZHANG; Hanbo, <b>XU; Jie</b>, KONG; Tao. DATA PROCESSING METHOD AND APPARATUS, DEVICE, AND COMPUTER MEDIUM: US-20250162161-A1[P]. 2025-05.</li>
									<li> Zhang; Hanbo, Li; Xinghang, Liu; Minghuan, <b>Xu; Jie</b>, Wu; Hongtao, Jing; Ya, Cheang; Chilam, Kong; Tao, Li; Hang. Information processing method, task execution method, apparatus, device and medium: US-12358135-B2[P]. 2025-07.</li>
									<li> ZHANG; Hanbo, LI; Xinghang, LIU; Minghuan, <b>XU; Jie</b>, WU; Hongtao, JING; Ya, CHEANG; Chilam, KONG; Tao, LI; Hang. INFORMATION PROCESSING METHOD, TASK EXECUTION METHOD, APPARATUS, DEVICE AND MEDIUM: US-20250018567-A1[P]. 2025-01.</li>
								</ol>
							</header>
							
							<footer class="major">
								<ul class="actions special">
									<li><a href="https://www.baiten.cn/so/list/(in%3A(%E8%AE%B8%E6%9D%B0))%20AND%20(%20pa%3A(%E8%A5%BF%E5%AE%89%E4%BA%A4%E9%80%9A%E5%A4%A7%E5%AD%A6)%20OR%20pa%3A(%20%E5%8C%97%E4%BA%AC%E6%9C%89%E7%AB%B9%E5%B1%85%E7%BD%91%E7%BB%9C%E6%8A%80%E6%9C%AF%E6%9C%89%E9%99%90%E5%85%AC%E5%8F%B8)).html?merge=no-merge&sort=DESC&sortField=score&pageSize=10&pageIndex=1&sc=35184372088831" class="icon brands bi-globe button">BAITEN (CN)</a></li>
									<li><a href="https://ppubs.uspto.gov/pubwebapp/static/pages/ppubsbasic.html" class="icon brands bi-globe button">USPTO(US)</a></li>
								</ul>
							</footer>
						</section>

					<!-- Get Started -->
						<section id="awa" class="main special">
							<header class="major">
								<h2>Awards</h2>
							</header>
							<div class="table-wrapper" style="text-align: left;">
								<table>
									<thead>
										<tr>
											<th>Time</th>
											<th>Title</th>
											<th>Prize</th>
										</tr>
									</thead>
									<tbody>
										<tr>
											<td>2024.5</td>
											<td>ICRA 2024 Workshop on Human-Robot Co-Manipulation</td>
											<td>Best Presentation Finalist</td>
										</tr>
										<tr>
											<td>2021.10</td>
											<td>西安交通大学学业奖学金</td>
											<td>一等奖</td>
										</tr>
										<tr>
											<td>2021.8</td>
											<td>世界机器人大赛（双臂协作组）</td>
											<td>一等奖</td>
										</tr>
										<tr>
											<td>2020.11</td>
											<td>阿里巴巴AI Lab - OCRTOC 机器人桌面整理竞赛</td>
											<td>仿真第四名</td>
										</tr>
										<tr>
											<td>2018.6</td>
											<td>湖南省普通高等学校优秀毕业生</td>
											<td>优秀毕业生</td>
										</tr>
										<tr>
											<td>2017.12</td>
											<td>全国大学生电子设计竞赛</td>
											<td>全国一等奖</td>
										</tr>
										<tr>
											<td>2017.6</td>
											<td>全国大学生工程训练综合能力竞赛</td>
											<td>全国二等奖</td>
										</tr>
									</tbody>
								</table>
							</div>
						</section>
					</div>

				<!-- Footer -->
					<footer id="footer">
						<section>
							<h2>License</h2>
							<p>Template by <a href="https://html5up.net">HTML5 UP</a>, licensed under <a href="https://html5up.net/license">CC BY 3.0</a>. </p>
							<ul class="actions">
								<li><a href="index.html" class="icon brands bi-person-circle button">About Me</a></li>
							</ul>

						</section>
						<section>
							<h2>Contact Me</h2>
							<dl class="alt">
								<dt>Address</dt>
								<dd>陕西省西安市咸宁西路28号 &bull; 710049 &bull; China</dd>
								<dt>Phone</dt>
								<dd>(+86) 185 **** 7096</dd>
								<dt>Email</dt>
								<dd><a href="mailto:xujiexx@gmail.com">xujiexx@gmail.com</a></dd>
							</dl>
							<ul class="icons">
								<li><a href="index.html" class="icon brands bi-globe alt"><span class="label">Website</span></a></li>
								<li><a href="mailto:xujiexx@gmail.com" class="icon brands bi-envelope alt"><span class="label">Email</span></a></li>
								<li><a href="https://huggingface.co/jxu124" class="icon brands bi-emoji-smile alt"><span class="label">Huggingface</span></a></li>
								<li><a href="https://github.com/jxu124" class="icon brands bi-github alt"><span class="label">GitHub</span></a></li>
								
							</ul>
						</section>
						<p class="copyright">&copy; Jie Xu - 2025</p>
					</footer>
			</div>

		<!-- Scripts -->
			<script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js"></script>
			<script src="https://cdn.jsdelivr.net/npm/jquery.scrollex@0.2.1/jquery.scrollex.min.js"></script>
			<script src="assets/js/jquery.min.js"></script>
			<script src="assets/js/jquery.scrollex.min.js"></script>
			<script src="assets/js/jquery.scrolly.min.js"></script>
			<script src="assets/js/browser.min.js"></script>
			<script src="assets/js/breakpoints.min.js"></script>
			<script src="assets/js/util.js"></script>
			<script src="assets/js/main.js"></script>
	</body>
</html>